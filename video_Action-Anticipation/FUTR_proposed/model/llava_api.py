import torch
import torch.nn as nn
import sys
import os
llava_model_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../awesome-llm/llava/"))
sys.path.append(llava_model_path)
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
#from transformers import VisionTextDualEncoderModel, AutoProcessor, VisionTextDualEncoderConfig, AutoConfig

class VideoToLabelLLM(nn.Module):
    def __init__(self, llm_model_name="liuhaotian/llava-v1.5-13b", max_length=4096, embedding_dim=512):
        super(VideoToLabelLLM, self).__init__()
        # Load LLaVA pretrained model, tokenizer, and image processor
        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
            model_path=llm_model_name,
            model_base=None,
            model_name=get_model_name_from_path(llm_model_name)
        )
        self.max_length = max_length
        # Ensure the tokenizer has a padding token
        if self.processor.tokenizer.pad_token is None:
            self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token

    def forward(self, video_embedding, human_prompt):
        """
        Args:
            video_embedding: Encoded video frames (list of PIL.Image or tensors).
            human_prompt: User-provided textual prompt.
        Returns:
            str: Fine-grained labels generated by the model.
        """
        # Process inputs (images and text)
        inputs = {
            "input_ids": self.tokenizer(
                human_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=self.max_length,
            ).input_ids.to(self.model.device),
            "pixel_values": self.image_processor(video_embedding, return_tensors="pt").pixel_values.to(self.model.device),
        }

        # Generate outputs
        outputs = self.model.generate(
            **inputs,
            max_length=300,  # Output sequence length
            do_sample=True,
            temperature=0.7
        )

        # Decode the result into text
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)


    # def forward(self, video_embedding, human_prompt):
    #     inputs = self.processor(
    #         images=video_embedding,
    #         text=human_prompt,
    #         return_tensors="pt",
    #         padding=True,
    #         #truncation=True,
    #         #max_length=self.max_length,
    #     ).to(self.llm.device)

    #     outputs = self.llm.generate(
    #         **inputs,
    #         max_length=300,
    #         do_sample=True,
    #         temperature=0.7
    #     )

    #     return self.processor.tokenizer.decode(outputs[0], skip_special_tokens=True)